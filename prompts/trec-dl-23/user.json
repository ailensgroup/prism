{
    "document": "\n**Objective:**\n   - Rank the retrieved DOCUMENTS by their likelihood of containing the best answer to the query, from MOST relevant (best first) to least relevant.\n**Task:**\n 1) Understand the Query\n   - Identify the user intent:\n       • Factoid (specific entity, date, number)\n       • Definition / explanation\n        • List / comparison\n         • Procedural / how-to\n         • Open-ended informational\n 2) Assess Document-Level Relevance\n    - Consider the document as a whole, not isolated sentences.\n    - Judge whether the document is likely to contain a complete, authoritative, or highly informative answer to the query.\n 3) Relevance Heuristics\n    - Prefer documents that:\n        • Directly address the core intent of the query\n        • Contain explicit answers or strong supporting evidence\n        • Are authoritative, comprehensive, and well-structured\n    - Penalize documents that are:\n        • Only tangentially related\n        • Overly broad with little focus on the query\n        • Mentions the topic without answering the question\n 4) Tie-Breaking Rules\n    - Break ties by prioritizing:\n        (1) Higher topical focus on the query\n        (2) Greater depth or completeness of coverage\n        (3) Clearer factual grounding or explanations\n 5) Output Format (STRICT)\n   - Return ONLY a Python list of document indices.\n   - Order them from most relevant to least relevant.\n   - Do NOT include explanations, text, or formatting.\n    - Example: [3, 1, 4, 0, 2]\n\n{icl_section}\n\n**Query:**\n{user_question}\n\n    **Candidate Documents:**\n    {documents}",
    "passage": "\n**Objective:**\n   - Rank the retrieved PASSAGES by their likelihood of directly answering the query, from MOST relevant (best first) to least relevant.\n\n    **Task:**\n    1) Understand the Query\n    - Determine what constitutes a correct answer:\n        • Exact fact or entity\n        • Short explanation or definition\n        • Evidence snippet supporting a claim\n        • Partial but directly useful information\n\n    2) Assess Passage-Level Relevance\n    - Judge each passage independently.\n    - Focus on whether the passage itself (without external context) provides a direct and useful answer to the query.\n\n    3) Relevance Heuristics\n    - Prefer passages that:\n        • Explicitly answer the question\n        • Contain key facts, definitions, or explanations\n        • Closely match the query terms and intent\n    - Penalize passages that:\n        • Are only topically related but do not answer\n        • Are too vague, generic, or background-only\n        • Require substantial inference beyond the text\n\n    4) Tie-Breaking Rules\n    - Break ties by prioritizing:\n        (1) Directness of the answer\n        (2) Specificity and factual clarity\n        (3) Minimal irrelevant content\n\n    5) Output Format (STRICT)\n    - Return ONLY a Python list of passage indices.\n    - Order them from most relevant to least relevant.\n    - Do NOT include explanations, text, or formatting.\n    - Example: [7, 2, 5, 1, 9]\n\n    {icl_section}\n\n    **Query:**\n    {user_question}\n\n    **Candidate Passages:**\n    {passages}"
}